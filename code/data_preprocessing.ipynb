{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs505_final_project_data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HChZsj2hyg6G"
      },
      "outputs": [],
      "source": [
        "!cp ./drive/MyDrive/cs505_final_project/NLP505Project-main.zip ./\n",
        "!unzip -q NLP505Project-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cHhhhSY7-17c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH0 = \"./NLP505Project-main/data/2017_Arabic_train_final/GOLD/\"\n",
        "PATH1 = \"./NLP505Project-main/data/2017_English_final/GOLD/Subtask_A/\"\n",
        "PATH2 = \"./NLP505Project-main/data/2017_English_final/GOLD/Subtasks_BD/\"\n",
        "PATH3 = \"./NLP505Project-main/data/2017_English_final/GOLD/Subtasks_CE/\"\n",
        "\n",
        "dict_path = dict()\n",
        "dict_path[\"arabic_a\"] = PATH0 + \"SemEval2017-task4-train.subtask-A.arabic.txt\"\n",
        "dict_path[\"arabic_bd\"] = PATH0 + \"SemEval2017-task4-train.subtask-BD.arabic.txt\"\n",
        "dict_path[\"arabic_ce\"] = PATH0 + \"SemEval2017-task4-train.subtask-CE.arabic.txt\"\n",
        "\n",
        "dict_path[\"english_a_13dev\"] = PATH1 + \"twitter-2013dev-A.txt\"\n",
        "dict_path[\"english_a_13test\"] = PATH1 + \"twitter-2013test-A.txt\"\n",
        "dict_path[\"english_a_13train\"] = PATH1 + \"twitter-2013train-A.txt\"\n",
        "dict_path[\"english_a_14sarcasm\"] = PATH1 + \"twitter-2014sarcasm-A.txt\"\n",
        "dict_path[\"english_a_14test\"] = PATH1 + \"twitter-2014test-A.txt\"\n",
        "dict_path[\"english_a_15test\"] = PATH1 + \"twitter-2015test-A.txt\"\n",
        "dict_path[\"english_a_15train\"] = PATH1 + \"twitter-2015train-A.txt\"\n",
        "dict_path[\"english_a_16dev\"] = PATH1 + \"twitter-2016dev-A.txt\"\n",
        "dict_path[\"english_a_16devtest\"] = PATH1 + \"twitter-2016devtest-A.txt\"\n",
        "dict_path[\"english_a_16test\"] = PATH1 + \"twitter-2016test-A.txt\"\n",
        "dict_path[\"english_a_16train\"] = PATH1 + \"twitter-2016train-A.txt\"\n",
        "\n",
        "dict_path[\"english_bd_15test\"] = PATH2 + \"twitter-2015testBD.txt\"\n",
        "dict_path[\"english_bd_15train\"] = PATH2 + \"twitter-2015train-BD.txt\"\n",
        "dict_path[\"english_bd_16dev\"] = PATH2 + \"twitter-2016dev-BD.txt\"\n",
        "dict_path[\"english_bd_16devtest\"] = PATH2 + \"twitter-2016devtest-BD.txt\"\n",
        "dict_path[\"english_bd_16test\"] = PATH2 + \"twitter-2016test-BD.txt\"\n",
        "dict_path[\"english_bd_16train\"] = PATH2 + \"twitter-2016train-BD.txt\"\n",
        "\n",
        "dict_path[\"english_ce_16dev\"] = PATH3 + \"twitter-2016dev-CE.txt\"\n",
        "dict_path[\"english_ce_16devtest\"] = PATH3 + \"twitter-2016devtest-CE.txt\"\n",
        "dict_path[\"english_ce_16test\"] = PATH3 + \"twitter-2016test-CE.txt\"\n",
        "dict_path[\"english_ce_16train\"] = PATH3 + \"twitter-2016train-CE.txt\"\n",
        "\n",
        "# ----- #\n",
        "# test dict_path\n",
        "# ----- #\n",
        "for key in dict_path.keys():\n",
        "    with open(dict_path[key], \"r\") as f:\n",
        "        f.read()"
      ],
      "metadata": {
        "id": "MAaq5YoC66XB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw(key):\n",
        "    with open(dict_path[key], \"r\") as f:\n",
        "        raw = f.read()\n",
        "        raw = [tmp.strip().split(\"\\t\") for tmp in raw.strip().split(\"\\n\")]\n",
        "        tmp = [len(tmp) for tmp in raw]\n",
        "        length_tmp = scipy.stats.mode(tmp)[0]\n",
        "        raw = [tmp for tmp in raw if len(tmp) == length_tmp]\n",
        "        raw = np.array(raw)\n",
        "    return raw\n",
        "\n",
        "\n",
        "def decode_raw(raw):\n",
        "    idx_text = raw.shape[1] - 1\n",
        "    idx_label = raw.shape[1] - 2\n",
        "    text = raw[:, idx_text]\n",
        "    label = raw[:, idx_label]\n",
        "    return text, label\n",
        "\n",
        "# ----- #\n",
        "# test decode_raw\n",
        "# ----- #\n",
        "for key in dict_path.keys():\n",
        "    raw = get_raw(key)\n",
        "    text, label = decode_raw(raw)\n",
        "    print(text[0:1])\n",
        "\n",
        "for key in dict_path.keys():\n",
        "    raw = get_raw(key)\n",
        "    text, label = decode_raw(raw)\n",
        "    print(label[0:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOkiPIaE2zsZ",
        "outputId": "228c0967-3ec1-491d-ad61-d19850effc76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"إجبار أبل على التعاون على فك شفرة اجهزتها https://t.co/bBRbxNe6qP\"']\n",
            "['\"رئيس شركة أبل: الواقع المعزز سيصبح أهم من الطعامhttps://t.co/VQyBwD3Sw4@RoayahNews https://t.co/EYj3ewE7JY\"']\n",
            "['\"إجبار أبل على التعاون على فك شفرة اجهزتها https://t.co/bBRbxNe6qP\"']\n",
            "['Won the match #getin . Plus\\\\u002c tomorrow is a very busy day\\\\u002c with Awareness Day\\\\u2019s and debates. Gulp. Debates...']\n",
            "['\"@jjuueellzz down in the Atlantic city, ventnor, margate, ocean city area. I\\'m just waiting for the coordinator to hopefully call me tomorrow\"']\n",
            "['Gas by my house hit $3.39!!!! I\\\\u2019m going to Chapel Hill on Sat. :)']\n",
            "['\"@MetroNorth wall to wall people on the platform at South Norwalk waiting for the 8:08. Thanks for the Sat. Sched. Great sense']\n",
            "['\"i\\'m done writing code for the week! Looks like we\\'ve developed a good a** game for the show Revenge on ABC Sunday, Premeres 09/30/12 9pm\"']\n",
            "[\"Saturday without Leeds United is like Sunday without a Sunday dinner it doesn't feel normal at all (Ryan)\"]\n",
            "['I forgot how sad the first episode of the 5th season of Dexter is. #depressing #dexter #darkpassenger']\n",
            "['05 Beat it - Michael Jackson - Thriller (25th Anniversary Edition) [HD] http://t.co/A4K2B86PBv']\n",
            "['@SeeMonterey LOST - Sony cell phone with holiday photos. Early Fri morning, Montreal transit plaza or no. 13 bus to airport. REWARD! Plz RT.']\n",
            "['\"Picturehouse\\'s, Pink Floyd\\'s, \\'Roger Waters: The Walll - opening 29 Sept is now making waves. Watch the trailer on Rolling Stone - look...\"']\n",
            "[\"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\"]\n",
            "[\"I just cut a 25 second audio clip of Aaron Rodgers talking about Jordy Nelson's grandma's pies. Happy Thursday.\"]\n",
            "['I forgot how sad the first episode of the 5th season of Dexter is. #depressing #dexter #darkpassenger']\n",
            "['Jay Z joins Instagram with nostalgic tribute to Michael Jackson: Jay Z apparently joined Instagram on Saturday and.. http://t.co/Qj9I4eCvXy']\n",
            "['@SeeMonterey LOST - Sony cell phone with holiday photos. Early Fri morning, Montreal transit plaza or no. 13 bus to airport. REWARD! Plz RT.']\n",
            "[\"@MargaretsBelly Amy Schumer is the stereotypical 1st world Laci Green feminazi. Plus she's unfunny\"]\n",
            "[\"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\"]\n",
            "['05 Beat it - Michael Jackson - Thriller (25th Anniversary Edition) [HD] http://t.co/A4K2B86PBv']\n",
            "['@SeeMonterey LOST - Sony cell phone with holiday photos. Early Fri morning, Montreal transit plaza or no. 13 bus to airport. REWARD! Plz RT.']\n",
            "[\"@MargaretsBelly Amy Schumer is the stereotypical 1st world Laci Green feminazi. Plus she's unfunny\"]\n",
            "[\"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\"]\n",
            "['positive' 'positive' 'positive' 'positive' 'neutral']\n",
            "['positive' 'positive' 'negative' 'negative' 'positive']\n",
            "['0' '0' '0' '1' '0']\n",
            "['neutral' 'neutral' 'negative' 'neutral' 'neutral']\n",
            "['positive' 'positive' 'neutral' 'negative' 'neutral']\n",
            "['positive' 'negative' 'negative' 'negative' 'neutral']\n",
            "['negative' 'neutral' 'negative' 'negative' 'negative']\n",
            "['positive' 'neutral' 'negative' 'positive' 'positive']\n",
            "['negative' 'positive' 'positive' 'positive' 'positive']\n",
            "['negative' 'negative' 'neutral' 'neutral' 'neutral']\n",
            "['neutral' 'positive' 'neutral' 'positive' 'positive']\n",
            "['neutral' 'neutral' 'negative' 'neutral' 'positive']\n",
            "['neutral' 'neutral' 'negative' 'neutral' 'positive']\n",
            "['negative' 'negative' 'negative' 'negative' 'neutral']\n",
            "['neutral' 'neutral' 'neutral' 'negative' 'neutral']\n",
            "['neutral' 'positive' 'positive' 'neutral' 'neutral']\n",
            "['positive' 'positive' 'positive' 'positive' 'positive']\n",
            "['positive' 'negative' 'positive' 'positive' 'negative']\n",
            "['negative' 'negative' 'negative' 'negative' 'negative']\n",
            "['negative' 'negative' 'negative' 'negative' 'positive']\n",
            "['0' '1' '0' '1' '1']\n",
            "['1' '0' '-1' '1' '1']\n",
            "['-1' '-1' '-1' '-1' '-1']\n",
            "['-1' '-2' '-1' '-1' '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in dict_path.keys():\n",
        "    raw = get_raw(key)\n",
        "    print(len(raw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcAeGSOadZya",
        "outputId": "ec23f375-2151-4e76-8df3-6c55047cfd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3355\n",
            "1656\n",
            "3355\n",
            "1654\n",
            "3547\n",
            "9684\n",
            "86\n",
            "1853\n",
            "2390\n",
            "489\n",
            "1999\n",
            "2000\n",
            "20622\n",
            "6000\n",
            "2383\n",
            "489\n",
            "1325\n",
            "1417\n",
            "10551\n",
            "4346\n",
            "2000\n",
            "2000\n",
            "20632\n",
            "6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ar, label_ar = decode_raw(get_raw(\"arabic_a\"))\n",
        "text_en, label_en = decode_raw(get_raw(\"english_a_16train\"))\n",
        "\n",
        "text_ar, label_ar = text_ar[:3000], label_ar[:3000]\n",
        "text_en, label_en = text_en[:3000], label_en[:3000]\n",
        "\n",
        "label_ar[label_ar == \"negative\"] = 0\n",
        "label_ar[label_ar == \"neutral\"] = 1\n",
        "label_ar[label_ar == \"positive\"] = 2\n",
        "label_ar = label_ar.astype(\"int32\")\n",
        "\n",
        "label_en[label_en == \"negative\"] = 0\n",
        "label_en[label_en == \"neutral\"] = 1\n",
        "label_en[label_en == \"positive\"] = 2\n",
        "label_en = label_en.astype(\"int32\")\n",
        "\n",
        "# create dataset_ar and dataset_en\n",
        "dataset_ar = np.stack((text_ar, label_ar), axis=1)\n",
        "dataset_en = np.stack((text_en, label_en), axis=1)\n",
        "\n",
        "# split training and test set\n",
        "dataset_ar_train = dataset_ar[:2100]\n",
        "dataset_ar_test = dataset_ar[2100:]\n",
        "dataset_en_train = dataset_en[:2100]\n",
        "dataset_en_test = dataset_en[2100:]\n",
        "\n",
        "# create dataset_mix_train and dataset_mix_test\n",
        "dataset_mix_train = np.concatenate((dataset_ar_train, dataset_en_train), axis=0)\n",
        "dataset_mix_test = np.concatenate((dataset_ar_test, dataset_en_test), axis=0)\n",
        "\n",
        "# shuffle\n",
        "np.random.shuffle(dataset_ar_train)\n",
        "np.random.shuffle(dataset_ar_test)\n",
        "np.random.shuffle(dataset_en_train)\n",
        "np.random.shuffle(dataset_en_test)\n",
        "np.random.shuffle(dataset_mix_train)\n",
        "np.random.shuffle(dataset_mix_test)\n"
      ],
      "metadata": {
        "id": "d2hjSdjraMvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ar_train = pd.DataFrame(dataset_ar_train)\n",
        "df_ar_train.columns = [\"text\", \"label\"]\n",
        "df_ar_train.to_csv(\"./dataset_ar_train.csv\", index=False)\n",
        "\n",
        "df_ar_test = pd.DataFrame(dataset_ar_test)\n",
        "df_ar_test.columns = [\"text\", \"label\"]\n",
        "df_ar_test.to_csv(\"./dataset_ar_test.csv\", index=False)\n",
        "\n",
        "df_en_train = pd.DataFrame(dataset_en_train)\n",
        "df_en_train.columns = [\"text\", \"label\"]\n",
        "df_en_train.to_csv(\"./dataset_en_train.csv\", index=False)\n",
        "\n",
        "df_en_test = pd.DataFrame(dataset_en_test)\n",
        "df_en_test.columns = [\"text\", \"label\"]\n",
        "df_en_test.to_csv(\"./dataset_en_test.csv\", index=False)\n",
        "\n",
        "df_mix_train = pd.DataFrame(dataset_mix_train)\n",
        "df_mix_train.columns = [\"text\", \"label\"]\n",
        "df_mix_train.to_csv(\"./dataset_mix_train.csv\", index=False)\n",
        "\n",
        "df_mix_test = pd.DataFrame(dataset_mix_test)\n",
        "df_mix_test.columns = [\"text\", \"label\"]\n",
        "df_mix_test.to_csv(\"./dataset_mix_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "slakQfkXi-yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p8aYGCrbjhSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len() + len(g) + len(get_raw(\"english_a_16test\")) + len(get_raw(\"english_a_16train\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee593fd4-ab51-44f3-e317-cc62c9f5f371",
        "id": "6Q131KNrBGjl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30621"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataset_en_train and dataset_en_test\n",
        "raw_en = np.concatenate((get_raw(\"english_a_16dev\"),\n",
        "                         get_raw(\"english_a_16devtest\"),\n",
        "                         get_raw(\"english_a_16test\"),\n",
        "                         get_raw(\"english_a_16train\")), axis=0)\n",
        "np.random.shuffle(raw_en)  # shuffle\n",
        "raw_en = raw_en[:30000]\n",
        "text_en, label_en = decode_raw(raw_en)\n",
        "\n",
        "label_en[label_en == \"negative\"] = 0\n",
        "label_en[label_en == \"neutral\"] = 1\n",
        "label_en[label_en == \"positive\"] = 2\n",
        "label_en = label_en.astype(\"int32\")\n",
        "\n",
        "dataset_en = np.stack((text_en, label_en), axis=1)\n",
        "dataset_en_train = dataset_en[:21000]\n",
        "dataset_en_test = dataset_en[21000:]"
      ],
      "metadata": {
        "id": "qXqciwJhBUXu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataset_ar_train and dataset_ar_test\n",
        "raw_ar = np.array(pd.read_csv(\"train_arabic.csv\"))\n",
        "np.random.shuffle(raw_ar)  # shuffle\n",
        "\n",
        "raw_ar = raw_ar[:30000]\n",
        "text_ar, label_ar = raw_ar[:, 0], raw_ar[:, 1]\n",
        "\n",
        "label_ar[label_ar == -1] = 0\n",
        "label_ar[label_ar == 0] = 1\n",
        "label_ar[label_ar == 1] = 2\n",
        "label_ar = label_ar.astype(\"int32\")\n",
        "\n",
        "dataset_ar = np.stack((text_ar, label_ar), axis=1)\n",
        "dataset_ar_train = dataset_ar[:21000]\n",
        "dataset_ar_test = dataset_ar[21000:]"
      ],
      "metadata": {
        "id": "G-k6IkNCCpOb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset_mix_train and dataset_mix_test\n",
        "dataset_mix_train = np.concatenate((dataset_ar_train, dataset_en_train), axis=0)\n",
        "dataset_mix_test = np.concatenate((dataset_ar_test, dataset_en_test), axis=0)\n",
        "\n",
        "# shuffle\n",
        "np.random.shuffle(dataset_mix_train)\n",
        "np.random.shuffle(dataset_mix_test)"
      ],
      "metadata": {
        "id": "Ej022mHKE_4y"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ar_train = pd.DataFrame(dataset_ar_train)\n",
        "df_ar_train.columns = [\"text\", \"label\"]\n",
        "df_ar_train.to_csv(\"./dataset_ar_train.csv\", index=False)\n",
        "\n",
        "df_ar_test = pd.DataFrame(dataset_ar_test)\n",
        "df_ar_test.columns = [\"text\", \"label\"]\n",
        "df_ar_test.to_csv(\"./dataset_ar_test.csv\", index=False)\n",
        "\n",
        "df_en_train = pd.DataFrame(dataset_en_train)\n",
        "df_en_train.columns = [\"text\", \"label\"]\n",
        "df_en_train.to_csv(\"./dataset_en_train.csv\", index=False)\n",
        "\n",
        "df_en_test = pd.DataFrame(dataset_en_test)\n",
        "df_en_test.columns = [\"text\", \"label\"]\n",
        "df_en_test.to_csv(\"./dataset_en_test.csv\", index=False)\n",
        "\n",
        "df_mix_train = pd.DataFrame(dataset_mix_train)\n",
        "df_mix_train.columns = [\"text\", \"label\"]\n",
        "df_mix_train.to_csv(\"./dataset_mix_train.csv\", index=False)\n",
        "\n",
        "df_mix_test = pd.DataFrame(dataset_mix_test)\n",
        "df_mix_test.columns = [\"text\", \"label\"]\n",
        "df_mix_test.to_csv(\"./dataset_mix_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "zaFL7o-iFOLR"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- #\n",
        "# history\n",
        "# ----- #\n",
        "# text_ar, label_ar = decode_raw(get_raw(\"arabic_a\"))\n",
        "# text_en, label_en = decode_raw(get_raw(\"english_a_16train\"))\n",
        "\n",
        "# text_ar, label_ar = text_ar[:3000], label_ar[:3000]\n",
        "# text_en, label_en = text_en[:3000], label_en[:3000]\n",
        "\n",
        "# label_ar[label_ar == \"negative\"] = 0\n",
        "# label_ar[label_ar == \"neutral\"] = 1\n",
        "# label_ar[label_ar == \"positive\"] = 2\n",
        "# label_ar = label_ar.astype(\"int32\")\n",
        "\n",
        "# label_en[label_en == \"negative\"] = 0\n",
        "# label_en[label_en == \"neutral\"] = 1\n",
        "# label_en[label_en == \"positive\"] = 2\n",
        "# label_en = label_en.astype(\"int32\")\n",
        "\n",
        "# # create dataset_ar and dataset_en\n",
        "# dataset_ar = np.stack((text_ar, label_ar), axis=1)\n",
        "# dataset_en = np.stack((text_en, label_en), axis=1)\n",
        "\n",
        "# # split training and test set\n",
        "# dataset_ar_train = dataset_ar[:2100]\n",
        "# dataset_ar_test = dataset_ar[2100:]\n",
        "# dataset_en_train = dataset_en[:2100]\n",
        "# dataset_en_test = dataset_en[2100:]\n",
        "\n",
        "# # create dataset_mix_train and dataset_mix_test\n",
        "# dataset_mix_train = np.concatenate((dataset_ar_train, dataset_en_train), axis=0)\n",
        "# dataset_mix_test = np.concatenate((dataset_ar_test, dataset_en_test), axis=0)\n",
        "\n",
        "# # shuffle\n",
        "# np.random.shuffle(dataset_ar_train)\n",
        "# np.random.shuffle(dataset_ar_test)\n",
        "# np.random.shuffle(dataset_en_train)\n",
        "# np.random.shuffle(dataset_en_test)\n",
        "# np.random.shuffle(dataset_mix_train)\n",
        "# np.random.shuffle(dataset_mix_test)\n"
      ],
      "metadata": {
        "id": "mJrWYcFJBWAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"./dataset_big.zip\" \"./dataset_big\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPeJ-z_yF6vT",
        "outputId": "bb0ef83e-c608-42f9-a716-29c39ecd21bf"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: dataset_big/ (stored 0%)\n",
            "  adding: dataset_big/dataset_mix_test.csv (deflated 55%)\n",
            "  adding: dataset_big/dataset_mix_train.csv (deflated 55%)\n",
            "  adding: dataset_big/dataset_ar_test.csv (deflated 67%)\n",
            "  adding: dataset_big/dataset_ar_train.csv (deflated 67%)\n",
            "  adding: dataset_big/dataset_en_test.csv (deflated 54%)\n",
            "  adding: dataset_big/dataset_en_train.csv (deflated 54%)\n"
          ]
        }
      ]
    }
  ]
}